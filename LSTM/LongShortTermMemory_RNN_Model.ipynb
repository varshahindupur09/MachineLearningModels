{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e82c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m ipykernel install --user --name=.venv\n",
    "# !pip install keras\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f37145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45065c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_topic_modeling(url):\n",
    "    # Step 1: Fetch the web page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Step 2: Parse the HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    class_name = \"content\"\n",
    "    research_div = soup.find('div', {'class': class_name})\n",
    "\n",
    "    # Extract text from the <div> tag\n",
    "    research_text = re.sub(r'<.*?>', '', str(research_div))\n",
    "    \n",
    "    # Preprocess and tokenize the research text\n",
    "    remove_words = ['for', 'and', 'research', 'explores','group', 'focuses', 'of', 'the', 'to', 'in', 'as', 'to', 'this', 'is', 'not', 'we', 'a', 'his', 'on'\n",
    "                    'impact', 'environment', 'environmental', 'pollution', 'includes', 'air', 'pollution', 'water', 'soil', 'are', 'indispensable', 'every', 'lakes', 'pollutant', 'untreated', 'thereby',\n",
    "                     'law', 'energy', 'degradation', 'by', 'former', 'harmful', 'molecule', 'modify', 'into', 'electronic', 'such', 'dyes', 'design']\n",
    "    \n",
    "    research_text = ' '.join([word for word in re.findall(r'\\w+', research_text.lower()) if word not in remove_words])\n",
    "\n",
    "    return research_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e89fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessed research areas as a single text\n",
    "url = \"https://facultyweb.kennesaw.edu/bbaruah/research.php\"\n",
    "research_text = function_topic_modeling(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c1e8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and vectorize the text using Keras Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([research_text])\n",
    "research_sequences = tokenizer.texts_to_sequences([research_text])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29227d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of n-grams for input to LSTM\n",
    "sequence_length = 10\n",
    "sequences = []\n",
    "for i in range(sequence_length, len(research_sequences[0])):\n",
    "    sequences.append(research_sequences[0][i - sequence_length:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c093663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = X[:, -1]\n",
    "X = X[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335c190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=sequence_length - 1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dae3317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 - 7s - loss: 5.8401 - accuracy: 0.0023 - 7s/epoch - 514ms/step\n",
      "Epoch 2/50\n",
      "14/14 - 0s - loss: 5.8326 - accuracy: 0.0207 - 258ms/epoch - 18ms/step\n",
      "Epoch 3/50\n",
      "14/14 - 0s - loss: 5.8226 - accuracy: 0.0161 - 238ms/epoch - 17ms/step\n",
      "Epoch 4/50\n",
      "14/14 - 0s - loss: 5.7613 - accuracy: 0.0138 - 240ms/epoch - 17ms/step\n",
      "Epoch 5/50\n",
      "14/14 - 0s - loss: 5.4972 - accuracy: 0.0161 - 249ms/epoch - 18ms/step\n",
      "Epoch 6/50\n",
      "14/14 - 0s - loss: 5.1946 - accuracy: 0.0230 - 266ms/epoch - 19ms/step\n",
      "Epoch 7/50\n",
      "14/14 - 0s - loss: 4.9071 - accuracy: 0.0368 - 287ms/epoch - 20ms/step\n",
      "Epoch 8/50\n",
      "14/14 - 0s - loss: 4.6419 - accuracy: 0.0483 - 344ms/epoch - 25ms/step\n",
      "Epoch 9/50\n",
      "14/14 - 0s - loss: 4.4021 - accuracy: 0.0506 - 309ms/epoch - 22ms/step\n",
      "Epoch 10/50\n",
      "14/14 - 0s - loss: 4.1701 - accuracy: 0.0782 - 289ms/epoch - 21ms/step\n",
      "Epoch 11/50\n",
      "14/14 - 0s - loss: 3.9811 - accuracy: 0.1057 - 297ms/epoch - 21ms/step\n",
      "Epoch 12/50\n",
      "14/14 - 0s - loss: 3.7907 - accuracy: 0.1356 - 296ms/epoch - 21ms/step\n",
      "Epoch 13/50\n",
      "14/14 - 0s - loss: 3.6435 - accuracy: 0.1540 - 320ms/epoch - 23ms/step\n",
      "Epoch 14/50\n",
      "14/14 - 0s - loss: 3.4769 - accuracy: 0.1908 - 316ms/epoch - 23ms/step\n",
      "Epoch 15/50\n",
      "14/14 - 0s - loss: 3.3453 - accuracy: 0.2483 - 296ms/epoch - 21ms/step\n",
      "Epoch 16/50\n",
      "14/14 - 0s - loss: 3.1821 - accuracy: 0.3080 - 329ms/epoch - 23ms/step\n",
      "Epoch 17/50\n",
      "14/14 - 0s - loss: 3.0471 - accuracy: 0.3471 - 287ms/epoch - 21ms/step\n",
      "Epoch 18/50\n",
      "14/14 - 0s - loss: 2.9426 - accuracy: 0.3724 - 285ms/epoch - 20ms/step\n",
      "Epoch 19/50\n",
      "14/14 - 0s - loss: 2.8106 - accuracy: 0.4253 - 291ms/epoch - 21ms/step\n",
      "Epoch 20/50\n",
      "14/14 - 0s - loss: 2.7018 - accuracy: 0.4667 - 292ms/epoch - 21ms/step\n",
      "Epoch 21/50\n",
      "14/14 - 0s - loss: 2.6055 - accuracy: 0.5011 - 317ms/epoch - 23ms/step\n",
      "Epoch 22/50\n",
      "14/14 - 0s - loss: 2.5201 - accuracy: 0.5011 - 312ms/epoch - 22ms/step\n",
      "Epoch 23/50\n",
      "14/14 - 0s - loss: 2.4394 - accuracy: 0.5310 - 289ms/epoch - 21ms/step\n",
      "Epoch 24/50\n",
      "14/14 - 0s - loss: 2.3342 - accuracy: 0.5655 - 302ms/epoch - 22ms/step\n",
      "Epoch 25/50\n",
      "14/14 - 0s - loss: 2.2629 - accuracy: 0.5816 - 306ms/epoch - 22ms/step\n",
      "Epoch 26/50\n",
      "14/14 - 0s - loss: 2.1937 - accuracy: 0.6414 - 296ms/epoch - 21ms/step\n",
      "Epoch 27/50\n",
      "14/14 - 0s - loss: 2.1490 - accuracy: 0.6046 - 321ms/epoch - 23ms/step\n",
      "Epoch 28/50\n",
      "14/14 - 0s - loss: 2.0949 - accuracy: 0.6299 - 391ms/epoch - 28ms/step\n",
      "Epoch 29/50\n",
      "14/14 - 0s - loss: 2.0273 - accuracy: 0.6782 - 320ms/epoch - 23ms/step\n",
      "Epoch 30/50\n",
      "14/14 - 0s - loss: 1.9304 - accuracy: 0.6989 - 307ms/epoch - 22ms/step\n",
      "Epoch 31/50\n",
      "14/14 - 0s - loss: 1.8786 - accuracy: 0.7126 - 369ms/epoch - 26ms/step\n",
      "Epoch 32/50\n",
      "14/14 - 0s - loss: 1.8042 - accuracy: 0.7310 - 295ms/epoch - 21ms/step\n",
      "Epoch 33/50\n",
      "14/14 - 0s - loss: 1.7353 - accuracy: 0.7678 - 297ms/epoch - 21ms/step\n",
      "Epoch 34/50\n",
      "14/14 - 1s - loss: 1.6796 - accuracy: 0.7747 - 568ms/epoch - 41ms/step\n",
      "Epoch 35/50\n",
      "14/14 - 0s - loss: 1.6214 - accuracy: 0.7954 - 370ms/epoch - 26ms/step\n",
      "Epoch 36/50\n",
      "14/14 - 0s - loss: 1.5662 - accuracy: 0.8046 - 305ms/epoch - 22ms/step\n",
      "Epoch 37/50\n",
      "14/14 - 0s - loss: 1.5421 - accuracy: 0.8069 - 276ms/epoch - 20ms/step\n",
      "Epoch 38/50\n",
      "14/14 - 0s - loss: 1.5198 - accuracy: 0.7931 - 305ms/epoch - 22ms/step\n",
      "Epoch 39/50\n",
      "14/14 - 0s - loss: 1.4718 - accuracy: 0.8046 - 285ms/epoch - 20ms/step\n",
      "Epoch 40/50\n",
      "14/14 - 0s - loss: 1.4077 - accuracy: 0.8368 - 307ms/epoch - 22ms/step\n",
      "Epoch 41/50\n",
      "14/14 - 0s - loss: 1.3704 - accuracy: 0.8322 - 338ms/epoch - 24ms/step\n",
      "Epoch 42/50\n",
      "14/14 - 0s - loss: 1.3268 - accuracy: 0.8345 - 311ms/epoch - 22ms/step\n",
      "Epoch 43/50\n",
      "14/14 - 0s - loss: 1.2944 - accuracy: 0.8506 - 337ms/epoch - 24ms/step\n",
      "Epoch 44/50\n",
      "14/14 - 0s - loss: 1.2574 - accuracy: 0.8552 - 318ms/epoch - 23ms/step\n",
      "Epoch 45/50\n",
      "14/14 - 0s - loss: 1.2229 - accuracy: 0.8690 - 311ms/epoch - 22ms/step\n",
      "Epoch 46/50\n",
      "14/14 - 0s - loss: 1.1842 - accuracy: 0.8736 - 297ms/epoch - 21ms/step\n",
      "Epoch 47/50\n",
      "14/14 - 0s - loss: 1.1486 - accuracy: 0.8713 - 275ms/epoch - 20ms/step\n",
      "Epoch 48/50\n",
      "14/14 - 0s - loss: 1.1042 - accuracy: 0.8851 - 273ms/epoch - 19ms/step\n",
      "Epoch 49/50\n",
      "14/14 - 0s - loss: 1.0660 - accuracy: 0.9080 - 272ms/epoch - 19ms/step\n",
      "Epoch 50/50\n",
      "14/14 - 0s - loss: 1.0518 - accuracy: 0.8920 - 270ms/epoch - 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea33003610>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f8fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Generated Topics:\n",
      "Topic 1: impact, on, among, these, primarily, related, release, christopher, rivers, several\n",
      "Topic 2: industries, food, drug, agyeman, rivers, tendency, michael, leather, cosmetics, extensively\n",
      "Topic 3: use, properties, oxygen, optical, optical, james, discharge, due, kristen, household\n",
      "Topic 4: conductivity, composite, material, sers, pose, primarily, technological, control, sub, craighead\n",
      "Topic 5: develop, electrically, 3, transparency, woods, concentrations, raman, downer, jeremiah, kelsey\n"
     ]
    }
   ],
   "source": [
    "# Generate topic predictions\n",
    "num_topics = 5\n",
    "num_words_per_topic = 10\n",
    "topics = []\n",
    "\n",
    "# Initial input sequence\n",
    "current_input = X[0]\n",
    "\n",
    "for _ in range(num_topics):\n",
    "    topic = []\n",
    "    for _ in range(num_words_per_topic):\n",
    "        # Predict next word\n",
    "        next_word = np.argmax(model.predict(np.array([current_input])))\n",
    "\n",
    "        # Append the predicted word to the topic\n",
    "        topic.append(list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(next_word)])\n",
    "\n",
    "        # Append the predicted word to the current input and reshape\n",
    "        current_input = np.append(current_input, next_word)\n",
    "        current_input = current_input[1:]\n",
    "\n",
    "    topics.append(topic)\n",
    "\n",
    "# Print generated topics\n",
    "print(\"Generated Topics:\")\n",
    "for idx, topic in enumerate(topics, start=1):\n",
    "    print(f\"Topic {idx}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6a484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
